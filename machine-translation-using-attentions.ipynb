{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Translation Using Attention:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to Hindi Translation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preparation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import all libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('../input/english-to-hindi-parallel-dataset/newdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                   english_sentence  \\\n",
       "0           0  politicians do not have permission to do what ...   \n",
       "1           1         I'd like to tell you about one such child,   \n",
       "2           2  This percentage is even greater than the perce...   \n",
       "3           3  what we really mean is that they're bad at not...   \n",
       "4           4  .The ending portion of these Vedas is called U...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  politicians do not have permission to do what ...   \n",
       "1         I'd like to tell you about one such child,   \n",
       "2  This percentage is even greater than the perce...   \n",
       "3  what we really mean is that they're bad at not...   \n",
       "4  .The ending portion of these Vedas is called U...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>177604</td>\n",
       "      <td>177606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>174076</td>\n",
       "      <td>147327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>(Laughter)</td>\n",
       "      <td>(हँसी)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>555</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       english_sentence hindi_sentence\n",
       "count            177604         177606\n",
       "unique           174076         147327\n",
       "top          (Laughter)         (हँसी)\n",
       "freq                555            212"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 177606 entries, 0 to 177605\n",
      "Data columns (total 2 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   english_sentence  177604 non-null  object\n",
      " 1   hindi_sentence    177606 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177604, 2)\n"
     ]
    }
   ],
   "source": [
    "data=data.dropna()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "and that is for their children to grow up successful,\n",
      "और वह है कि उनके बच्चे कामयाब निकलें,\n"
     ]
    }
   ],
   "source": [
    "n=int(input())\n",
    "\n",
    "en=data['english_sentence'].values[n]\n",
    "\n",
    "hi=data['hindi_sentence'].values[n]\n",
    "\n",
    "print(en)\n",
    "\n",
    "print(hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning The Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "sc = list(set(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'और वह है कि उनके बच्चे कामयाब निकलें,'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['english_sentence']=data['english_sentence'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['english_sentence', 'hindi_sentence'], dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['english_sentence']=data['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in sc))\n",
    "data['hindi_sentence']=data['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['english_sentence']=data['english_sentence'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))\n",
    "data['hindi_sentence']=data['hindi_sentence'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['english_sentence']=data['english_sentence'].apply(lambda x: '<start> '+x+' <end>')\n",
    "data['hindi_sentence']=data['hindi_sentence'].apply(lambda x: '<start> '+x+' <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length_eng_sentence']=data['english_sentence'].apply(lambda x:len(x.split(\" \")))\n",
    "data['length_hin_sentence']=data['hindi_sentence'].apply(lambda x:len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>length_eng_sentence</th>\n",
       "      <th>length_hin_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; politicians do not have permission to ...</td>\n",
       "      <td>&lt;start&gt; राजनीतिज्ञों के पास जो कार्य करना चाहि...</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; id like to tell you about one such chi...</td>\n",
       "      <td>&lt;start&gt; मई आपको ऐसे ही एक बच्चे के बारे में बत...</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; this percentage is even greater than t...</td>\n",
       "      <td>&lt;start&gt; यह प्रतिशत भारत में हिन्दुओं प्रतिशत स...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; what we really mean is that theyre bad...</td>\n",
       "      <td>&lt;start&gt; हम ये नहीं कहना चाहते कि वो ध्यान नहीं...</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; the ending portion of these vedas is c...</td>\n",
       "      <td>&lt;start&gt; इन्हीं वेदों का अंतिम भाग उपनिषद कहलात...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  <start> politicians do not have permission to ...   \n",
       "1  <start> id like to tell you about one such chi...   \n",
       "2  <start> this percentage is even greater than t...   \n",
       "3  <start> what we really mean is that theyre bad...   \n",
       "4  <start> the ending portion of these vedas is c...   \n",
       "\n",
       "                                      hindi_sentence  length_eng_sentence  \\\n",
       "0  <start> राजनीतिज्ञों के पास जो कार्य करना चाहि...                   14   \n",
       "1  <start> मई आपको ऐसे ही एक बच्चे के बारे में बत...                   11   \n",
       "2  <start> यह प्रतिशत भारत में हिन्दुओं प्रतिशत स...                   12   \n",
       "3  <start> हम ये नहीं कहना चाहते कि वो ध्यान नहीं...                   14   \n",
       "4  <start> इन्हीं वेदों का अंतिम भाग उपनिषद कहलात...                   11   \n",
       "\n",
       "   length_hin_sentence  \n",
       "0                   16  \n",
       "1                   13  \n",
       "2                   11  \n",
       "3                   13  \n",
       "4                   10  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fliter the values based upon length of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data['length_eng_sentence']<=20]\n",
    "data=data[data['length_hin_sentence']<=20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109012, 4)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "<start> compilation of bachans great poetry  <end>\n",
      "<start> बच्चन के श्रेष्ठ कविताओं का संकलन <end>\n"
     ]
    }
   ],
   "source": [
    "n=int(input())\n",
    "\n",
    "en=data['english_sentence'].values[n]\n",
    "\n",
    "hi=data['hindi_sentence'].values[n]\n",
    "\n",
    "print(en)\n",
    "\n",
    "print(hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* combine all words\n",
    "* sort the words based upon frequency \n",
    "* assign the ranks of the words based upon frequency\n",
    "* convert the text sentence into list of tokens\n",
    "* padding the token's list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "def tokenize(lang):\n",
    "    words=[]\n",
    "    for i in lang:\n",
    "        words.extend(i.split())\n",
    "    s=Counter(words)\n",
    "    a=list(s.keys())\n",
    "    b=list(s.values())\n",
    "    ind=np.argsort(np.array(b))\n",
    "    word_to_ind={}\n",
    "    for i in range(len(ind)):\n",
    "        word_to_ind[a[ind[-(i+1)]]]=i+1\n",
    "    sequences=[]\n",
    "    for i in lang:\n",
    "        sen=[]\n",
    "        for j in i.split():\n",
    "            sen.append(word_to_ind[j])\n",
    "        sequences.append(sen)\n",
    "    pad_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences,padding='post')\n",
    "    \n",
    "    return word_to_ind,pad_sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_to_ind,en_sequences=tokenize(data['english_sentence'].values)\n",
    "hin_word_to_ind,hin_sequences=tokenize(data['hindi_sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47181, 53372)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_word_to_ind),len(hin_word_to_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((109012, 20), (109012, 20))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sequences.shape,hin_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sequences[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split The data into train and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87209 87209 21803 21803\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(en_sequences,hin_sequences, test_size=0.2)\n",
    "\n",
    "\n",
    "print(len(x_train), len(y_train), len(x_val), len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shuffle data and use Data Generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(x_train)\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = len(x_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_inp_size = len(en_word_to_ind)+1\n",
    "vocab_tar_size = len(hin_word_to_ind)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention MOdel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enoder of Attention Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder of Attention Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Optimizer and Loss Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define checkpoint to store the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '/kaggle/working/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([hin_word_to_ind['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "           \n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.0041\n",
      "Epoch 1 Batch 100 Loss 1.0714\n",
      "Epoch 1 Batch 200 Loss 1.0046\n",
      "Epoch 1 Batch 300 Loss 1.0268\n",
      "Epoch 1 Batch 400 Loss 1.0905\n",
      "Epoch 1 Batch 500 Loss 1.1144\n",
      "Epoch 1 Batch 600 Loss 1.1082\n",
      "Epoch 1 Loss 1.0768\n",
      "Time taken for 1 epoch 334.2789969444275 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.8810\n",
      "Epoch 2 Batch 100 Loss 0.8535\n",
      "Epoch 2 Batch 200 Loss 0.9281\n",
      "Epoch 2 Batch 300 Loss 0.9610\n",
      "Epoch 2 Batch 400 Loss 1.0462\n",
      "Epoch 2 Batch 500 Loss 0.8626\n",
      "Epoch 2 Batch 600 Loss 0.9808\n",
      "Epoch 2 Loss 0.9638\n",
      "Time taken for 1 epoch 333.14415216445923 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.8228\n",
      "Epoch 3 Batch 100 Loss 0.9114\n",
      "Epoch 3 Batch 200 Loss 0.9229\n",
      "Epoch 3 Batch 300 Loss 0.9736\n",
      "Epoch 3 Batch 400 Loss 0.7572\n",
      "Epoch 3 Batch 500 Loss 0.8735\n",
      "Epoch 3 Batch 600 Loss 0.9038\n",
      "Epoch 3 Loss 0.8660\n",
      "Time taken for 1 epoch 331.37976360321045 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.7407\n",
      "Epoch 4 Batch 100 Loss 0.7864\n",
      "Epoch 4 Batch 200 Loss 0.7146\n",
      "Epoch 4 Batch 300 Loss 0.7955\n",
      "Epoch 4 Batch 400 Loss 0.7588\n",
      "Epoch 4 Batch 500 Loss 0.8164\n",
      "Epoch 4 Batch 600 Loss 0.7594\n",
      "Epoch 4 Loss 0.7781\n",
      "Time taken for 1 epoch 333.5558183193207 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.6885\n",
      "Epoch 5 Batch 100 Loss 0.6718\n",
      "Epoch 5 Batch 200 Loss 0.6756\n",
      "Epoch 5 Batch 300 Loss 0.7320\n",
      "Epoch 5 Batch 400 Loss 0.7496\n",
      "Epoch 5 Batch 500 Loss 0.7069\n",
      "Epoch 5 Batch 600 Loss 0.6813\n",
      "Epoch 5 Loss 0.6988\n",
      "Time taken for 1 epoch 334.1917335987091 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.5658\n",
      "Epoch 6 Batch 100 Loss 0.6032\n",
      "Epoch 6 Batch 200 Loss 0.6984\n",
      "Epoch 6 Batch 300 Loss 0.6320\n",
      "Epoch 6 Batch 400 Loss 0.6494\n",
      "Epoch 6 Batch 500 Loss 0.6379\n",
      "Epoch 6 Batch 600 Loss 0.6687\n",
      "Epoch 6 Loss 0.6315\n",
      "Time taken for 1 epoch 333.8512668609619 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.5394\n",
      "Epoch 7 Batch 100 Loss 0.5514\n",
      "Epoch 7 Batch 200 Loss 0.6119\n",
      "Epoch 7 Batch 300 Loss 0.5151\n",
      "Epoch 7 Batch 400 Loss 0.5917\n",
      "Epoch 7 Batch 500 Loss 0.5469\n",
      "Epoch 7 Batch 600 Loss 0.5477\n",
      "Epoch 7 Loss 0.5678\n",
      "Time taken for 1 epoch 332.06087589263916 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.4589\n",
      "Epoch 8 Batch 100 Loss 0.5063\n",
      "Epoch 8 Batch 200 Loss 0.5538\n",
      "Epoch 8 Batch 300 Loss 0.5626\n",
      "Epoch 8 Batch 400 Loss 0.4382\n",
      "Epoch 8 Batch 500 Loss 0.4444\n",
      "Epoch 8 Batch 600 Loss 0.5552\n",
      "Epoch 8 Loss 0.5087\n",
      "Time taken for 1 epoch 331.60308623313904 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.4172\n",
      "Epoch 9 Batch 100 Loss 0.4833\n",
      "Epoch 9 Batch 200 Loss 0.4272\n",
      "Epoch 9 Batch 300 Loss 0.4047\n",
      "Epoch 9 Batch 400 Loss 0.4784\n",
      "Epoch 9 Batch 500 Loss 0.5322\n",
      "Epoch 9 Batch 600 Loss 0.5216\n",
      "Epoch 9 Loss 0.4577\n",
      "Time taken for 1 epoch 334.58924674987793 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.4487\n",
      "Epoch 10 Batch 100 Loss 0.4330\n",
      "Epoch 10 Batch 200 Loss 0.3782\n",
      "Epoch 10 Batch 300 Loss 0.4352\n",
      "Epoch 10 Batch 400 Loss 0.4320\n",
      "Epoch 10 Batch 500 Loss 0.4169\n",
      "Epoch 10 Batch 600 Loss 0.4522\n",
      "Epoch 10 Loss 0.4153\n",
      "Time taken for 1 epoch 336.41129183769226 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch,batch_loss.numpy()))\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Of the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "hin_ind_to_word={}\n",
    "\n",
    "for i in hin_word_to_ind:\n",
    "    hin_ind_to_word[hin_word_to_ind[i]]=i\n",
    "    \n",
    "en_ind_to_word={}\n",
    "\n",
    "for i in en_word_to_ind:\n",
    "    en_ind_to_word[en_word_to_ind[i]]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    x=sentence.lower()\n",
    "    x=''.join(ch for ch in x if ch not in sc)\n",
    "    x=''.join([i for i in x if not i.isdigit()])\n",
    "    x='<start> '+x+' <end>'\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((20, 20))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "   \n",
    "    inputs = [en_word_to_ind[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=20,padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, 512))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([hin_word_to_ind['<start>']], 0)\n",
    "\n",
    "    for t in range(20):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        \n",
    "\n",
    "        if hin_ind_to_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        result += hin_ind_to_word[predicted_id] + ' '\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "english sentence---> peach is great for and dries up very well\n",
      "\n",
      "\n",
      "predicted sentence--->आडू चेहरे के लिए श्रेष्‍ठ भलीभाँति सूखता है । \n",
      "\n",
      "\n",
      "actual sentence--> जरदालू चेहरे के लिए श्रेष्‍ठ तथा भलीभाँति सूखता है\n",
      "\n",
      "\n",
      "--------------------------------------\n",
      "18\n",
      "english sentence---> on being hit by heatstroke feeding onion will get benefits\n",
      "\n",
      "\n",
      "predicted sentence--->जले व्यक्‍ति को नष्‍ट करने से रोगी को लाभ पहुँचाती है । \n",
      "\n",
      "\n",
      "actual sentence--> लू लगने पर प्याज का सेवन करायें तो लाभ मिलेगा\n",
      "\n",
      "\n",
      "--------------------------------------\n",
      "278\n",
      "english sentence---> penis is an external genital organ of male of some of the vertebrate and invertebrate both\n",
      "\n",
      "\n",
      "predicted sentence--->शिश्न Penis कशेरुकी और उसके पीछे दोनों से संबंधित कऋ से संबंधित कऋ से संबंधित कऋ से संबंधित कुछ खास \n",
      "\n",
      "\n",
      "actual sentence--> शिश्न Penis कशेरुकी और अकशेरुकी दोनो प्रकार के कुछ नर जीवों का एक बाह्य यौन अंग\n",
      "\n",
      "\n",
      "--------------------------------------\n",
      "897\n",
      "english sentence---> hitler had overrun almost the whole of western europe and italy had joined the war\n",
      "\n",
      "\n",
      "predicted sentence--->यूरोप के कई देशों की अधिकांश लोग युद्ध की थी। \n",
      "\n",
      "\n",
      "actual sentence--> हिटलर लगभग पूरे पश्चिमी यूरोप को कुचल चुका था और इटली भी युद्ध में कूद\n",
      "\n",
      "\n",
      "--------------------------------------\n",
      "5432\n",
      "english sentence---> i came to the most remote place that i could think of\n",
      "\n",
      "\n",
      "predicted sentence--->मैं सबसे बड़ा स्थानों पर जाने का था \n",
      "\n",
      "\n",
      "actual sentence--> मैं ऐसी जगह पर आया जो मेरी कल्पना में सबसे दूरदराज़ में\n",
      "\n",
      "\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    k=int(input())\n",
    "    sentence=''\n",
    "    for j in range(1,len(x_val[k])-1):\n",
    "        if  x_val[k][j+1]==0:\n",
    "            continue\n",
    "        sentence+=en_ind_to_word[x_val[k][j]]+' '\n",
    "    \n",
    "    pred,x,atten_plot=evaluate(sentence.strip())\n",
    "    actual=''\n",
    "    for j in range(1,len(y_val[k])-1):\n",
    "        if  x_val[k][j+1]==0:\n",
    "            continue\n",
    "        \n",
    "        actual+=' '+hin_ind_to_word[y_val[k][j]]\n",
    "    x=' '.join([j for j in x.split()[1:-1]])       \n",
    "    print(\"english sentence---> \"+x)\n",
    "    print('\\n')\n",
    "    print('predicted sentence--->'+pred)\n",
    "    print('\\n')\n",
    "    print('actual sentence-->'+actual)\n",
    "    print('\\n')\n",
    "    print('--------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
